# Fine-tuning Other Models with VLA-Arena Generated Datasets

VLA-Arena provides a complete framework for collecting data, converting data formats, and evaluating vision-language-action models. This guide will help you understand how to fine-tune VLA models using datasets generated by VLA-Arena.

## Quick Start

If you already have your dataset and OpenVLA model ready, you can start fine-tuning directly with the following commands:

### Standard OpenVLA Fine-tuning

```bash
# 1. Activate environment
conda activate openvla

# 2. Run fine-tuning script
./vla-scripts/finetune_openvla.sh \
    --dataset_name "your_dataset" \
    --vla_path "/path/to/your/openvla/model" \
    --data_root_dir "/path/to/your/datasets" \
    --openvla_root_dir "/path/to/openvla/repo"
```

### OpenVLA OFT Fine-tuning (Recommended)

```bash
# 1. Activate environment
conda activate openvla

# 2. Run OFT fine-tuning script
./vla-scripts/finetune_openvla_oft.sh \
    --dataset_name "your_dataset" \
    --vla_path "/path/to/your/openvla/model" \
    --data_root_dir "/path/to/your/datasets" \
    --openvla_root_dir "/path/to/openvla/repo"
```

### UniVLA Fine-tuning

```bash
# 1. Activate environment
conda activate univla

# 2. Run UniVLA fine-tuning script
./vla-scripts/finetune_univla.sh \
    --dataset_name "your_dataset" \
    --vla_path "/path/to/your/univla/model" \
    --lam_path "/path/to/your/lam/checkpoint" \
    --data_root_dir "/path/to/your/datasets" \
    --univla_root_dir "/path/to/univla/repo"
```

For detailed usage instructions, please refer to the sections below.

## Table of Contents

1. [Quick Start](#quick-start)
2. [Fine-tuning OpenVLA](#fine-tuning-openvla)
   - [Installing OpenVLA Library](#installing-openvla-library)
   - [One-click Fine-tuning with Scripts](#one-click-fine-tuning-with-scripts)
     - [Basic Usage](#basic-usage)
     - [Required Parameters](#required-parameters)
     - [Optional Parameters](#optional-parameters)
     - [Dataset Configuration Parameters](#dataset-configuration-parameters)
     - [State and Action Encoding Options](#state-and-action-encoding-options)
     - [Usage Examples](#usage-examples)
     - [Script Features](#script-features)
     - [Notes](#notes)
3. [Fine-tuning OpenVLA OFT](#fine-tuning-openvla-oft)
   - [OFT Fine-tuning Introduction](#oft-fine-tuning-introduction)
   - [Using OFT Script for Fine-tuning](#using-oft-script-for-fine-tuning)
     - [Basic Usage](#basic-usage-1)
     - [Required Parameters](#required-parameters-1)
     - [Basic Training Parameters](#basic-training-parameters)
     - [LoRA Parameters](#lora-parameters)
     - [Action Representation Parameters](#action-representation-parameters)
     - [Architecture Options](#architecture-options)
     - [Learning Rate Scheduling](#learning-rate-scheduling)
     - [Validation and Checkpoints](#validation-and-checkpoints)
     - [Logging Configuration](#logging-configuration)
     - [Dataset Configuration Parameters](#dataset-configuration-parameters-1)
     - [GPU Configuration](#gpu-configuration)
     - [Usage Examples](#usage-examples-1)
     - [Script Features](#script-features-1)
     - [Notes](#notes-1)
4. [Fine-tuning UniVLA](#fine-tuning-univla)
   - [Installing UniVLA Library](#installing-univla-library)
   - [One-click Fine-tuning with Scripts](#one-click-fine-tuning-with-scripts-1)
     - [Basic Usage](#basic-usage-2)
     - [Required Parameters](#required-parameters-2)
     - [Basic Training Parameters](#basic-training-parameters-1)
     - [LoRA Parameters](#lora-parameters-1)
     - [UniVLA Specific Parameters](#univla-specific-parameters)
     - [LAM Parameters](#lam-parameters)
     - [Logging Configuration](#logging-configuration-1)
     - [Dataset Configuration Parameters](#dataset-configuration-parameters-2)
     - [GPU Configuration](#gpu-configuration-1)
     - [Usage Examples](#usage-examples-2)
     - [Script Features](#script-features-2)
     - [Notes](#notes-2)
5. [Fine-tuning OpenPi](#fine-tuning-openpi)
   - [Installing OpenPi Library](#installing-openpi-library)
   - [One-click Fine-tuning with Scripts](#one-click-fine-tuning-with-scripts-2)
     - [Basic Usage](#basic-usage-3)
     - [Required Parameters](#required-parameters-3)
     - [Model Configuration Parameters](#model-configuration-parameters)
     - [Training Parameters](#training-parameters)
     - [Dataset Configuration Parameters](#dataset-configuration-parameters-3)
     - [Usage Examples](#usage-examples-3)
     - [Script Features](#script-features-3)
     - [Notes](#notes-3)
6. [Model Evaluation](#model-evaluation)
7. [Adding Custom Models](#adding-custom-models)
8. [Configuration Instructions](#configuration-instructions)

## Fine-tuning OpenVLA

### Installing OpenVLA Library

```bash
# Create and activate conda environment
conda create -n openvla python=3.10 -y
conda activate openvla

# Install PyTorch. Below is a sample command to do this, but you should check the following link
# to find installation instructions that are specific to your compute platform:
# https://pytorch.org/get-started/locally/
conda install pytorch torchvision torchaudio pytorch-cuda=12.4 -c pytorch -c nvidia -y  # UPDATE ME!

# Clone and install the openvla repo
git clone https://github.com/openvla/openvla.git
cd openvla
pip install -e .

# Install Flash Attention 2 for training (https://github.com/Dao-AILab/flash-attention)
#   =>> If you run into difficulty, try `pip cache remove flash_attn` first
pip install packaging ninja
ninja --version; echo $?  # Verify Ninja --> should return exit code "0"
pip install "flash-attn==2.5.5" --no-build-isolation
```

### One-click Fine-tuning with Scripts

Copy [finetune_openvla.sh](./finetune_openvla.sh) to the openvla/vla-scripts directory. This script will automatically add dataset configuration and run fine-tuning.

#### Basic Usage

```bash
# Activate conda environment
conda activate openvla

# Basic usage (requires providing required parameters)
./vla-scripts/finetune_openvla.sh \
    --dataset_name "my_dataset" \
    --vla_path "/path/to/openvla/model" \
    --data_root_dir "/path/to/datasets" \
    --openvla_root_dir "/path/to/openvla/repo"

# Custom parameters
./vla-scripts/finetune_openvla.sh \
    --dataset_name "my_dataset" \
    --vla_path "/path/to/openvla/model" \
    --data_root_dir "/path/to/datasets" \
    --openvla_root_dir "/path/to/openvla/repo" \
    --batch_size 4 \
    --learning_rate 1e-4 \
    --max_steps 10000 \
    --wandb_project "my_project"
```

#### Required Parameters

- `--dataset_name`: Dataset name (required)
- `--vla_path`: OpenVLA model path (required)
- `--data_root_dir`: Dataset root directory (required)
- `--openvla_root_dir`: OpenVLA repository root directory (required)

#### Optional Parameters

- `--run_root_dir`: Directory to save run results (default: `new_runs`)
- `--batch_size`: Batch size (default: `2`)
- `--learning_rate`: Learning rate (default: `5e-4`)
- `--max_steps`: Maximum training steps (default: `50000`)
- `--use_lora`: Whether to use LoRA fine-tuning (default: `true`)
- `--lora_rank`: LoRA rank (default: `32`)
- `--use_quantization`: Whether to use quantization (default: `false`)
- `--image_aug`: Whether to use image augmentation (default: `true`)
- `--wandb_project`: WandB project name (default: `safe-openvla`)
- `--wandb_entity`: WandB entity name (default: `trial`)
- `--num_gpus`: Number of GPUs to use (default: `1`)

#### Dataset Configuration Parameters

The script will automatically add your dataset configuration to `configs.py` and `transforms.py` files. You can customize dataset configuration:

- `--image_obs_primary`: Primary image observation key (default: `image`)
- `--image_obs_secondary`: Secondary image observation key (default: empty)
- `--image_obs_wrist`: Wrist image observation key (default: `wrist_image`)
- `--depth_obs_primary`: Primary depth observation key (default: empty)
- `--depth_obs_secondary`: Secondary depth observation key (default: empty)
- `--depth_obs_wrist`: Wrist depth observation key (default: empty)
- `--state_obs_keys`: State observation keys (default: `EEF_state,None,gripper_state`)
- `--state_encoding`: State encoding (default: `POS_EULER`)
- `--action_encoding`: Action encoding (default: `EEF_POS`)

#### State and Action Encoding Options

**State Encoding**:
- `NONE`: No proprioceptive state
- `POS_EULER`: EEF XYZ (3) + Roll-Pitch-Yaw (3) + <PAD> (1) + Gripper state (1)
- `POS_QUAT`: EEF XYZ (3) + Quaternion (4) + Gripper state (1)
- `JOINT`: Joint angles (7, padded with <PAD> if insufficient) + Gripper state (1)
- `JOINT_BIMANUAL`: Joint angles (2 x [ Joint angles (6) + Gripper state (1) ])

**Action Encoding**:
- `EEF_POS`: EEF delta XYZ (3) + Roll-Pitch-Yaw (3) + Gripper state (1)
- `JOINT_POS`: Joint delta position (7) + Gripper state (1)
- `JOINT_POS_BIMANUAL`: Joint delta position (2 x [ Joint delta position (6) + Gripper state (1) ])
- `EEF_R6`: EEF delta XYZ (3) + R6 (6) + Gripper state (1)

#### Usage Examples

**Example 1: Basic Usage**
```bash
./vla-scripts/finetune_openvla.sh \
    --dataset_name "my_robot_dataset" \
    --vla_path "/path/to/openvla/model" \
    --data_root_dir "/path/to/datasets" \
    --openvla_root_dir "/path/to/openvla/repo"
```

**Example 2: Custom Configuration**
```bash
./vla-scripts/finetune_openvla.sh \
    --dataset_name "custom_dataset" \
    --vla_path "/path/to/openvla/model" \
    --data_root_dir "/path/to/datasets" \
    --openvla_root_dir "/path/to/openvla/repo" \
    --image_obs_primary "front_camera" \
    --image_obs_wrist "gripper_camera" \
    --state_obs_keys "joint_positions,None,gripper_state" \
    --batch_size 8 \
    --learning_rate 1e-4 \
    --max_steps 20000
```

**Example 3: Using Quantization**
```bash
./vla-scripts/finetune_openvla.sh \
    --dataset_name "quantized_dataset" \
    --vla_path "/path/to/openvla/model" \
    --data_root_dir "/path/to/datasets" \
    --openvla_root_dir "/path/to/openvla/repo" \
    --use_quantization true \
    --batch_size 16 \
    --max_steps 5000
```

#### Script Features

1. **Parameter Validation**: Checks if required parameters are provided
2. **Add Dataset Configuration**: Automatically adds your dataset configuration to:
   - `{openvla_root_dir}/prismatic/vla/datasets/rlds/oxe/configs.py`
   - `{openvla_root_dir}/prismatic/vla/datasets/rlds/oxe/transforms.py`
3. **Run Fine-tuning**: Executes OpenVLA fine-tuning script with your parameters

#### Notes

- The script uses `libero_dataset_transform` as the default transform function for new datasets
- If dataset configuration already exists, the add configuration step will be skipped
- The script automatically handles `None` values in state observation keys
- Ensure your dataset is in the correct RLDS format and located in the specified data directory

## Fine-tuning OpenVLA OFT

### OFT Fine-tuning Introduction

OpenVLA OFT (Open-source Foundation Transformers) fine-tuning provides more advanced training options and better performance. The OFT version supports:

- **Richer Training Parameters**: Including learning rate scheduling, gradient accumulation, validation sets, etc.
- **Action Representation Options**: Supporting L1 regression and diffusion modeling
- **Architecture Enhancements**: FiLM language fusion, multi-image input, proprioceptive state, etc.
- **Advanced Optimization**: LoRA dropout, LoRA merging during training, etc.

### Using OFT Script for Fine-tuning

Copy [finetune_openvla_oft.sh](./finetune_openvla_oft.sh) to the openvla/vla-scripts directory. This script provides more comprehensive fine-tuning options.

#### Basic Usage

```bash
# Activate conda environment
conda activate openvla

# Basic usage (requires providing required parameters)
./vla-scripts/finetune_openvla_oft.sh \
    --dataset_name "my_dataset" \
    --vla_path "/path/to/openvla/model" \
    --data_root_dir "/path/to/datasets" \
    --openvla_root_dir "/path/to/openvla/repo"

# Custom parameters
./vla-scripts/finetune_openvla_oft.sh \
    --dataset_name "my_dataset" \
    --vla_path "/path/to/openvla/model" \
    --data_root_dir "/path/to/datasets" \
    --openvla_root_dir "/path/to/openvla/repo" \
    --batch_size 8 \
    --learning_rate 1e-4 \
    --max_steps 100000 \
    --use_l1_regression true \
    --use_film true
```

#### Required Parameters

- `--dataset_name`: Dataset name (required)
- `--vla_path`: OpenVLA model path (required)
- `--data_root_dir`: Dataset root directory (required)
- `--openvla_root_dir`: OpenVLA repository root directory (required)

#### Basic Training Parameters

- `--run_root_dir`: Directory to save run results (default: `all_runs`)
- `--batch_size`: Batch size (default: `7`)
- `--learning_rate`: Learning rate (default: `5e-4`)
- `--max_steps`: Maximum training steps (default: `150000`)
- `--grad_accumulation_steps`: Gradient accumulation steps (default: `1`)
- `--shuffle_buffer_size`: Data loader shuffle buffer size (default: `100000`)

#### LoRA Parameters

- `--use_lora`: Whether to use LoRA fine-tuning (default: `true`)
- `--lora_rank`: LoRA rank (default: `32`)
- `--lora_dropout`: LoRA dropout (default: `0.0`)
- `--merge_lora_during_training`: Merge LoRA during training (default: `true`)

#### Action Representation Parameters

- `--use_l1_regression`: Use L1 regression (default: `true`)
- `--use_diffusion`: Use diffusion modeling (default: `false`)
- `--num_diffusion_steps_train`: Training diffusion steps (default: `50`)
- `--diffusion_sample_freq`: Diffusion sampling frequency (default: `50`)

#### Architecture Options

- `--use_film`: Use FiLM for language fusion (default: `true`)
- `--num_images_in_input`: Number of images in input (default: `2`)
- `--use_proprio`: Include proprioceptive state (default: `false`)
- `--use_quantization`: Use quantization (default: `false`)
- `--image_aug`: Use image augmentation (default: `true`)

#### Learning Rate Scheduling

- `--lr_warmup_steps`: Learning rate warmup steps (default: `0`)
- `--num_steps_before_decay`: Steps before learning rate decay (default: `60000`)

#### Validation and Checkpoints

- `--use_val_set`: Use validation set (default: `false`)
- `--val_freq`: Validation frequency (default: `10000`)
- `--val_time_limit`: Validation time limit (default: `180`)
- `--save_freq`: Save frequency (default: `5000`)
- `--save_latest_checkpoint_only`: Save only latest checkpoint (default: `false`)
- `--resume`: Resume from checkpoint (default: `false`)
- `--resume_step`: Resume step (default: empty)

#### Logging Configuration

- `--wandb_project`: WandB project name (default: `openvla-oft-workflow-generalization`)
- `--wandb_entity`: WandB entity name (default: `trial`)
- `--wandb_log_freq`: WandB logging frequency (default: `10`)

#### Dataset Configuration Parameters

The script will automatically add your dataset configuration to `configs.py` and `transforms.py` files. You can customize dataset configuration:

- `--image_obs_primary`: Primary image observation key (default: `image`)
- `--image_obs_secondary`: Secondary image observation key (default: empty)
- `--image_obs_wrist`: Wrist image observation key (default: `wrist_image`)
- `--depth_obs_primary`: Primary depth observation key (default: empty)
- `--depth_obs_secondary`: Secondary depth observation key (default: empty)
- `--depth_obs_wrist`: Wrist depth observation key (default: empty)
- `--state_obs_keys`: State observation keys (default: `EEF_state,None,gripper_state`)
- `--state_encoding`: State encoding (default: `POS_EULER`)
- `--action_encoding`: Action encoding (default: `EEF_POS`)

#### GPU Configuration

- `--num_gpus`: Number of GPUs to use (default: `1`)

#### Usage Examples

**Example 1: Basic OFT Usage**
```bash
./vla-scripts/finetune_openvla_oft.sh \
    --dataset_name "my_robot_dataset" \
    --vla_path "/path/to/openvla/model" \
    --data_root_dir "/path/to/datasets" \
    --openvla_root_dir "/path/to/openvla/repo"
```

**Example 2: Advanced OFT Configuration**
```bash
./vla-scripts/finetune_openvla_oft.sh \
    --dataset_name "advanced_dataset" \
    --vla_path "/path/to/openvla/model" \
    --data_root_dir "/path/to/datasets" \
    --openvla_root_dir "/path/to/openvla/repo" \
    --batch_size 8 \
    --learning_rate 1e-4 \
    --max_steps 100000 \
    --use_l1_regression true \
    --use_film true \
    --use_proprio true \
    --num_images_in_input 3 \
    --lora_rank 64 \
    --grad_accumulation_steps 2
```

**Example 3: Using Diffusion Modeling**
```bash
./vla-scripts/finetune_openvla_oft.sh \
    --dataset_name "diffusion_dataset" \
    --vla_path "/path/to/openvla/model" \
    --data_root_dir "/path/to/datasets" \
    --openvla_root_dir "/path/to/openvla/repo" \
    --use_diffusion true \
    --num_diffusion_steps_train 100 \
    --diffusion_sample_freq 25 \
    --batch_size 4
```

**Example 4: Multi-GPU Training**
```bash
./vla-scripts/finetune_openvla_oft.sh \
    --dataset_name "multi_gpu_dataset" \
    --vla_path "/path/to/openvla/model" \
    --data_root_dir "/path/to/datasets" \
    --openvla_root_dir "/path/to/openvla/repo" \
    --num_gpus 4 \
    --batch_size 16 \
    --grad_accumulation_steps 1
```

#### Script Features

1. **Parameter Validation**: Checks if required parameters are provided
2. **Add Dataset Configuration**: Automatically adds your dataset configuration to:
   - `{openvla_root_dir}/prismatic/vla/datasets/rlds/oxe/configs.py`
   - `{openvla_root_dir}/prismatic/vla/datasets/rlds/oxe/transforms.py`
3. **Run OFT Fine-tuning**: Executes OpenVLA OFT fine-tuning script with your parameters
4. **Multi-GPU Support**: Supports multi-GPU distributed training

#### Notes

- The OFT version provides richer training options, suitable for users who need fine control over the training process
- Supports diffusion modeling, suitable for scenarios requiring generative action prediction
- FiLM language fusion can provide better language-visual interaction
- Multi-image input supports multi-view robot tasks
- Ensure your hardware resources are sufficient to support the selected training configuration

## Fine-tuning UniVLA

### Installing UniVLA Library

```bash
# Create and activate conda environment
conda create -n univla python=3.10 -y
conda activate univla

# Install PyTorch. Below is a sample command to do this, but you should check the following link
# to find installation instructions that are specific to your compute platform:
# https://pytorch.org/get-started/locally/
conda install pytorch torchvision torchaudio pytorch-cuda=12.4 -c pytorch -c nvidia -y  # UPDATE ME!

# Clone and install the univla repo
git clone https://github.com/opendrivelab/UniVLA.git
cd UniVLA
pip install -e .

# Install Flash Attention 2 for training (https://github.com/Dao-AILab/flash-attention)
#   =>> If you run into difficulty, try `pip cache remove flash_attn` first
pip install packaging ninja
ninja --version; echo $?  # Verify Ninja --> should return exit code "0"
pip install "flash-attn==2.5.5" --no-build-isolation

# Install additional dependencies for UniVLA
pip install swanlab
pip install ema-pytorch
pip install peft
pip install accelerate
```

### One-click Fine-tuning with Scripts

Copy [finetune_univla.sh](./finetune_univla.sh) to the UniVLA/vla-scripts directory. This script will automatically add dataset configuration and run fine-tuning.

#### Basic Usage

```bash
# Activate conda environment
conda activate univla

# Basic usage (requires providing required parameters)
./vla-scripts/finetune_univla.sh \
    --dataset_name "my_dataset" \
    --vla_path "/path/to/univla/model" \
    --lam_path "/path/to/lam/checkpoint" \
    --data_root_dir "/path/to/datasets" \
    --univla_root_dir "/path/to/univla/repo"

# Custom parameters
./vla-scripts/finetune_univla.sh \
    --dataset_name "my_dataset" \
    --vla_path "/path/to/univla/model" \
    --lam_path "/path/to/lam/checkpoint" \
    --data_root_dir "/path/to/datasets" \
    --univla_root_dir "/path/to/univla/repo" \
    --batch_size 4 \
    --learning_rate 1e-4 \
    --max_steps 50000 \
    --wandb_project "my_project"
```

#### Required Parameters

- `--dataset_name`: Dataset name (required)
- `--vla_path`: UniVLA model path (required)
- `--lam_path`: LAM (Latent Action Model) checkpoint path (required)
- `--data_root_dir`: Dataset root directory (required)
- `--univla_root_dir`: UniVLA repository root directory (required)

#### Basic Training Parameters

- `--run_root_dir`: Directory to save run results (default: `all_runs`)
- `--batch_size`: Batch size (default: `8`)
- `--learning_rate`: Learning rate (default: `3.5e-4`)
- `--max_steps`: Maximum training steps (default: `100000`)
- `--save_steps`: Save interval (default: `10000`)
- `--grad_accumulation_steps`: Gradient accumulation steps (default: `2`)
- `--shuffle_buffer_size`: Data loader shuffle buffer size (default: `16000`)

#### LoRA Parameters

- `--use_lora`: Whether to use LoRA fine-tuning (default: `true`)
- `--lora_rank`: LoRA rank (default: `32`)
- `--lora_dropout`: LoRA dropout (default: `0.0`)
- `--use_quantization`: Whether to use quantization (default: `false`)

#### UniVLA Specific Parameters

- `--freeze_vla`: Freeze VLA backbone (default: `false`)
- `--save_latest_checkpoint_only`: Save only latest checkpoint (default: `true`)
- `--run_id_note`: Extra note for experiment ID (default: empty)

#### LAM Parameters

UniVLA uses a Latent Action Model (LAM) for action representation. These parameters control the LAM architecture:

- `--codebook_size`: LAM codebook size (default: `16`)
- `--lam_model_dim`: LAM model dimension (default: `768`)
- `--lam_latent_dim`: LAM latent dimension (default: `128`)
- `--lam_patch_size`: LAM patch size (default: `14`)
- `--lam_enc_blocks`: LAM encoder blocks (default: `12`)
- `--lam_dec_blocks`: LAM decoder blocks (default: `12`)
- `--lam_num_heads`: LAM number of heads (default: `12`)
- `--window_size`: Action window size (default: `12`)

#### Logging Configuration

- `--wandb_project`: WandB project name (default: `finetune-UniVLA`)
- `--wandb_entity`: WandB entity name (default: `opendrivelab`)

#### Dataset Configuration Parameters

The script will automatically add your dataset configuration to `configs.py` and `transforms.py` files. You can customize dataset configuration:

- `--image_obs_primary`: Primary image observation key (default: `image`)
- `--image_obs_secondary`: Secondary image observation key (default: empty)
- `--image_obs_wrist`: Wrist image observation key (default: `wrist_image`)
- `--depth_obs_primary`: Primary depth observation key (default: empty)
- `--depth_obs_secondary`: Secondary depth observation key (default: empty)
- `--depth_obs_wrist`: Wrist depth observation key (default: empty)
- `--state_obs_keys`: State observation keys (default: `EEF_state,None,gripper_state`)
- `--state_encoding`: State encoding (default: `POS_EULER`)
- `--action_encoding`: Action encoding (default: `EEF_POS`)

#### GPU Configuration

- `--num_gpus`: Number of GPUs to use (default: `1`)

#### Usage Examples

**Example 1: Basic Usage**
```bash
./vla-scripts/finetune_univla.sh \
    --dataset_name "my_robot_dataset" \
    --vla_path "/path/to/univla/model" \
    --lam_path "/path/to/lam/checkpoint" \
    --data_root_dir "/path/to/datasets" \
    --univla_root_dir "/path/to/univla/repo"
```

**Example 2: Custom Configuration**
```bash
./vla-scripts/finetune_univla.sh \
    --dataset_name "custom_dataset" \
    --vla_path "/path/to/univla/model" \
    --lam_path "/path/to/lam/checkpoint" \
    --data_root_dir "/path/to/datasets" \
    --univla_root_dir "/path/to/univla/repo" \
    --image_obs_primary "front_camera" \
    --image_obs_wrist "gripper_camera" \
    --state_obs_keys "joint_positions,None,gripper_state" \
    --batch_size 4 \
    --learning_rate 1e-4 \
    --max_steps 50000 \
    --window_size 16
```

**Example 3: Using Quantization**
```bash
./vla-scripts/finetune_univla.sh \
    --dataset_name "quantized_dataset" \
    --vla_path "/path/to/univla/model" \
    --lam_path "/path/to/lam/checkpoint" \
    --data_root_dir "/path/to/datasets" \
    --univla_root_dir "/path/to/univla/repo" \
    --use_quantization true \
    --batch_size 16 \
    --max_steps 25000
```

**Example 4: Freeze VLA Backbone**
```bash
./vla-scripts/finetune_univla.sh \
    --dataset_name "frozen_vla_dataset" \
    --vla_path "/path/to/univla/model" \
    --lam_path "/path/to/lam/checkpoint" \
    --data_root_dir "/path/to/datasets" \
    --univla_root_dir "/path/to/univla/repo" \
    --freeze_vla true \
    --learning_rate 1e-3 \
    --batch_size 12
```

**Example 5: Multi-GPU Training**
```bash
./vla-scripts/finetune_univla.sh \
    --dataset_name "multi_gpu_dataset" \
    --vla_path "/path/to/univla/model" \
    --lam_path "/path/to/lam/checkpoint" \
    --data_root_dir "/path/to/datasets" \
    --univla_root_dir "/path/to/univla/repo" \
    --num_gpus 4 \
    --batch_size 8 \
    --grad_accumulation_steps 1
```

#### Script Features

1. **Parameter Validation**: Checks if required parameters are provided
2. **Add Dataset Configuration**: Automatically adds your dataset configuration to:
   - `{univla_root_dir}/prismatic/vla/datasets/rlds/oxe/configs.py`
   - `{univla_root_dir}/prismatic/vla/datasets/rlds/oxe/transforms.py`
3. **Run UniVLA Fine-tuning**: Executes UniVLA fine-tuning script with your parameters
4. **Multi-GPU Support**: Supports multi-GPU distributed training
5. **LAM Integration**: Automatically configures and loads the Latent Action Model

#### Notes

- UniVLA uses a two-stage training approach with a Latent Action Model (LAM)
- The LAM checkpoint is required and should be pre-trained
- The script uses `libero_dataset_transform` as the default transform function for new datasets
- If dataset configuration already exists, the add configuration step will be skipped
- The script automatically handles `None` values in state observation keys
- Ensure your dataset is in the correct RLDS format and located in the specified data directory
- UniVLA supports both frozen and unfrozen VLA backbone training

## Fine-tuning OpenPi

### Installing OpenPi Library

```bash
# Clone repository (with submodules)
git clone --recurse-submodules git@github.com:Physical-Intelligence/openpi.git

# Or if you have already cloned the repository:
cd openpi
git submodule update --init --recursive

# Install uv (if not already installed)
curl -LsSf https://astral.sh/uv/install.sh | sh

# Install OpenPi
cd openpi
GIT_LFS_SKIP_SMUDGE=1 uv sync
GIT_LFS_SKIP_SMUDGE=1 uv pip install -e .
```

**Note:** `GIT_LFS_SKIP_SMUDGE=1` is required to skip LFS file downloads for LeRobot dependencies.

### One-click Fine-tuning with Scripts

Copy [finetune_openpi.sh](./finetune_openpi.sh) to the openpi/scripts directory. This script will automatically add training configuration and run fine-tuning.

#### Basic Usage

```bash
# Basic usage (requires providing required parameters)
uv run bash scripts/finetune_openpi.sh \
    --config_name "my_openpi_config" \
    --exp_name "my_experiment" \
    --base_checkpoint_path "/path/to/base/checkpoint" \
    --dataset_repo_id "your_dataset_repo" \
    --hf_lerobot_home "/path/to/lerobot/home"

# Custom parameters
uv run bash scripts/finetune_openpi.sh \
    --config_name "custom_config" \
    --exp_name "custom_experiment" \
    --base_checkpoint_path "/path/to/base/checkpoint" \
    --dataset_repo_id "your_dataset_repo" \
    --hf_lerobot_home "/path/to/lerobot/home" \
    --model_type "pi0_fast" \
    --batch_size 32 \
    --learning_rate 1e-4 \
    --num_train_steps 50000
```

#### Required Parameters

- `--config_name`: Configuration name (required)
- `--exp_name`: Experiment name (required)
- `--base_checkpoint_path`: Base model checkpoint path (required)
- `--dataset_repo_id`: Dataset repository ID (required)
- `--hf_lerobot_home`: HF_LEROBOT_HOME directory path (required)

#### Model Configuration Parameters

- `--model_type`: Model type, pi0 or pi0_fast (default: pi0)
- `--action_dim`: Action dimension (default: 7)
- `--action_horizon`: Action time horizon (default: 10)
- `--max_token_len`: Maximum token length (default: 180)
- `--use_lora`: Use LoRA fine-tuning (default: false)
- `--lora_rank`: LoRA rank (default: 32)
- `--lora_dropout`: LoRA dropout (default: 0.0)
- `--paligemma_variant`: Paligemma variant (default: gemma_2b)
- `--action_expert_variant`: Action expert variant (default: gemma_300m)

#### Training Parameters

- `--batch_size`: Batch size (default: 56)
- `--learning_rate`: Learning rate (default: 3.5e-4)
- `--num_train_steps`: Training steps (default: 30000)
- `--log_interval`: Log interval (default: 100)
- `--save_interval`: Save interval (default: 1000)
- `--keep_period`: Keep period (default: 5000)
- `--num_workers`: Number of workers (default: 2)
- `--seed`: Random seed (default: 42)
- `--fsdp_devices`: FSDP devices (default: 1)
- `--ema_decay`: EMA decay (default: 0.99)

#### Dataset Configuration Parameters

- `--prompt_from_task`: Get prompt from task (default: true)

#### Usage Examples

**Example 1: Basic Usage**
```bash
uv run bash scripts/finetune_openpi.sh \
    --config_name "libero_pi0" \
    --exp_name "libero_experiment" \
    --base_checkpoint_path "/path/to/pi0/checkpoint" \
    --dataset_repo_id "libero_dataset" \
    --hf_lerobot_home "/path/to/lerobot/home"
```

**Example 2: Using pi0_fast Model**
```bash
uv run bash scripts/finetune_openpi.sh \
    --config_name "libero_pi0_fast" \
    --exp_name "libero_fast_experiment" \
    --base_checkpoint_path "/path/to/pi0_fast/checkpoint" \
    --dataset_repo_id "libero_dataset" \
    --hf_lerobot_home "/path/to/lerobot/home" \
    --model_type "pi0_fast" \
    --batch_size 32 \
    --learning_rate 1e-4
```

**Example 3: Using LoRA Fine-tuning**
```bash
uv run bash scripts/finetune_openpi.sh \
    --config_name "libero_pi0_lora" \
    --exp_name "libero_lora_experiment" \
    --base_checkpoint_path "/path/to/pi0/checkpoint" \
    --dataset_repo_id "libero_dataset" \
    --hf_lerobot_home "/path/to/lerobot/home" \
    --use_lora true \
    --lora_rank 64 \
    --lora_dropout 0.1
```

**Example 4: Custom Training Parameters**
```bash
uv run bash scripts/finetune_openpi.sh \
    --config_name "custom_libero" \
    --exp_name "custom_experiment" \
    --base_checkpoint_path "/path/to/checkpoint" \
    --dataset_repo_id "libero_dataset" \
    --hf_lerobot_home "/path/to/lerobot/home" \
    --batch_size 64 \
    --learning_rate 2e-4 \
    --num_train_steps 100000 \
    --save_interval 2000 \
    --wandb_enabled true \
    --project_name "my_openpi_project"
```

#### Script Features

1. **Parameter Validation**: Checks if required parameters are provided
2. **Add Training Configuration**: Automatically adds your training configuration to `src/openpi/training/config.py`
3. **Compute Normalization Statistics**: Automatically runs `scripts/compute_norm_stats.py`
4. **Run Training**: Executes OpenPi training script with your parameters
5. **Support Override**: Option to override existing checkpoints

#### Notes

- The script uses `LeRobotLiberoDataConfig` as the dataset configuration
- If configuration already exists, the add configuration step will be skipped
- Supports both pi0 and pi0_fast model types
- LoRA fine-tuning automatically sets appropriate freeze filters
- Ensure base checkpoint path is valid and accessible
- Ensure dataset repository ID is correct and accessible
- The script automatically sets the `HF_LEROBOT_HOME` environment variable


